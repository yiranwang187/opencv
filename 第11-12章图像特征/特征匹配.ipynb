{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征匹配"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Brute-Force蛮力匹配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = cv2.imread('box.png', 0)\n",
    "img2 = cv2.imread('box_in_scene.png', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_show(name,img):\n",
    "    cv2.imshow(name, img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_show('img1',img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_show('img2',img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sift = cv2.xfeatures2d.SIFT_create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp1, des1 = sift.detectAndCompute(img1, None)\n",
    "kp2, des2 = sift.detectAndCompute(img2, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crossCheck表示两个特征点要互相匹，\n",
    "# 例如A中的第i个特征点与B中的第j个特征点最近的，\n",
    "# 并且B中的第j个特征点到A中的第i个特征点也是最近的\n",
    "# 默认使用Norm_l2\n",
    "#NORM_L2: 归一化数组的(欧几里德距离)，如果其他特征计算方法需要考虑不同的匹配计算方式\n",
    "# BFMatcher就是：Brute-Force蛮力匹配\n",
    "bf = cv2.BFMatcher(crossCheck=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1对1的匹配\n",
    "特征点一对一的匹配：一个特征点对应一个特征点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "392\n"
     ]
    }
   ],
   "source": [
    "# 对两类特征向量进行匹配\n",
    "matches = bf.match(des1, des2)\n",
    "# 并且排序\n",
    "matches = sorted(matches, key=lambda x: x.distance)\n",
    "\n",
    "print(type(matches))\n",
    "print(len(matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function match:\n",
      "\n",
      "match(...) method of cv2.BFMatcher instance\n",
      "    match(queryDescriptors, trainDescriptors[, mask]) -> matches\n",
      "    .   @brief Finds the best match for each descriptor from a query set.\n",
      "    .   \n",
      "    .   @param queryDescriptors Query set of descriptors.\n",
      "    .   @param trainDescriptors Train set of descriptors. This set is not added to the train descriptors\n",
      "    .   collection stored in the class object.\n",
      "    .   @param matches Matches. If a query descriptor is masked out in mask , no match is added for this\n",
      "    .   descriptor. So, matches size may be smaller than the query descriptors count.\n",
      "    .   @param mask Mask specifying permissible matches between an input query and train matrices of\n",
      "    .   descriptors.\n",
      "    .   \n",
      "    .   In the first variant of this method, the train descriptors are passed as an input argument. In the\n",
      "    .   second variant of the method, train descriptors collection that was set by DescriptorMatcher::add is\n",
      "    .   used. Optional mask (or masks) can be passed to specify which query and training descriptors can be\n",
      "    .   matched. Namely, queryDescriptors[i] can be matched with trainDescriptors[j] only if\n",
      "    .   mask.at\\<uchar\\>(i,j) is non-zero.\n",
      "    \n",
      "    \n",
      "    \n",
      "    match(queryDescriptors[, masks]) -> matches\n",
      "    .   @overload\n",
      "    .   @param queryDescriptors Query set of descriptors.\n",
      "    .   @param matches Matches. If a query descriptor is masked out in mask , no match is added for this\n",
      "    .   descriptor. So, matches size may be smaller than the query descriptors count.\n",
      "    .   @param masks Set of masks. Each masks[i] specifies permissible matches between the input query\n",
      "    .   descriptors and stored train descriptors from the i-th image trainDescCollection[i].\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(bf.match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function findHomography:\n",
      "\n",
      "findHomography(...)\n",
      "    findHomography(srcPoints, dstPoints[, method[, ransacReprojThreshold[, mask[, maxIters[, confidence]]]]]) -> retval, mask\n",
      "    .   @brief Finds a perspective transformation between two planes.\n",
      "    .   \n",
      "    .   @param srcPoints Coordinates of the points in the original plane, a matrix of the type CV_32FC2\n",
      "    .   or vector\\<Point2f\\> .\n",
      "    .   @param dstPoints Coordinates of the points in the target plane, a matrix of the type CV_32FC2 or\n",
      "    .   a vector\\<Point2f\\> .\n",
      "    .   @param method Method used to compute a homography matrix. The following methods are possible:\n",
      "    .   -   **0** - a regular method using all the points, i.e., the least squares method\n",
      "    .   -   **RANSAC** - RANSAC-based robust method\n",
      "    .   -   **LMEDS** - Least-Median robust method\n",
      "    .   -   **RHO** - PROSAC-based robust method\n",
      "    .   @param ransacReprojThreshold Maximum allowed reprojection error to treat a point pair as an inlier\n",
      "    .   (used in the RANSAC and RHO methods only). That is, if\n",
      "    .   \\f[\\| \\texttt{dstPoints} _i -  \\texttt{convertPointsHomogeneous} ( \\texttt{H} * \\texttt{srcPoints} _i) \\|_2  >  \\texttt{ransacReprojThreshold}\\f]\n",
      "    .   then the point \\f$i\\f$ is considered as an outlier. If srcPoints and dstPoints are measured in pixels,\n",
      "    .   it usually makes sense to set this parameter somewhere in the range of 1 to 10.\n",
      "    .   @param mask Optional output mask set by a robust method ( RANSAC or LMEDS ). Note that the input\n",
      "    .   mask values are ignored.\n",
      "    .   @param maxIters The maximum number of RANSAC iterations.\n",
      "    .   @param confidence Confidence level, between 0 and 1.\n",
      "    .   \n",
      "    .   The function finds and returns the perspective transformation \\f$H\\f$ between the source and the\n",
      "    .   destination planes:\n",
      "    .   \n",
      "    .   \\f[s_i  \\vecthree{x'_i}{y'_i}{1} \\sim H  \\vecthree{x_i}{y_i}{1}\\f]\n",
      "    .   \n",
      "    .   so that the back-projection error\n",
      "    .   \n",
      "    .   \\f[\\sum _i \\left ( x'_i- \\frac{h_{11} x_i + h_{12} y_i + h_{13}}{h_{31} x_i + h_{32} y_i + h_{33}} \\right )^2+ \\left ( y'_i- \\frac{h_{21} x_i + h_{22} y_i + h_{23}}{h_{31} x_i + h_{32} y_i + h_{33}} \\right )^2\\f]\n",
      "    .   \n",
      "    .   is minimized. If the parameter method is set to the default value 0, the function uses all the point\n",
      "    .   pairs to compute an initial homography estimate with a simple least-squares scheme.\n",
      "    .   \n",
      "    .   However, if not all of the point pairs ( \\f$srcPoints_i\\f$, \\f$dstPoints_i\\f$ ) fit the rigid perspective\n",
      "    .   transformation (that is, there are some outliers), this initial estimate will be poor. In this case,\n",
      "    .   you can use one of the three robust methods. The methods RANSAC, LMeDS and RHO try many different\n",
      "    .   random subsets of the corresponding point pairs (of four pairs each, collinear pairs are discarded), estimate the homography matrix\n",
      "    .   using this subset and a simple least-squares algorithm, and then compute the quality/goodness of the\n",
      "    .   computed homography (which is the number of inliers for RANSAC or the least median re-projection error for\n",
      "    .   LMeDS). The best subset is then used to produce the initial estimate of the homography matrix and\n",
      "    .   the mask of inliers/outliers.\n",
      "    .   \n",
      "    .   Regardless of the method, robust or not, the computed homography matrix is refined further (using\n",
      "    .   inliers only in case of a robust method) with the Levenberg-Marquardt method to reduce the\n",
      "    .   re-projection error even more.\n",
      "    .   \n",
      "    .   The methods RANSAC and RHO can handle practically any ratio of outliers but need a threshold to\n",
      "    .   distinguish inliers from outliers. The method LMeDS does not need any threshold but it works\n",
      "    .   correctly only when there are more than 50% of inliers. Finally, if there are no outliers and the\n",
      "    .   noise is rather small, use the default method (method=0).\n",
      "    .   \n",
      "    .   The function is used to find initial intrinsic and extrinsic matrices. Homography matrix is\n",
      "    .   determined up to a scale. Thus, it is normalized so that \\f$h_{33}=1\\f$. Note that whenever an \\f$H\\f$ matrix\n",
      "    .   cannot be estimated, an empty one will be returned.\n",
      "    .   \n",
      "    .   @sa\n",
      "    .   getAffineTransform, estimateAffine2D, estimateAffinePartial2D, getPerspectiveTransform, warpPerspective,\n",
      "    .   perspectiveTransform\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(cv2.findHomography)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 再进行显示\n",
    "img3 = cv2.drawMatches(img1, kp1, img2, kp2, matches[:20], None,flags=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_show('img3',img3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k对最佳匹配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "603\n",
      "2\n",
      "336\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "bf = cv2.BFMatcher()\n",
    "# 一个点对应k个点也可\n",
    "# 为每个关键点绘制两条匹配线\n",
    "matches = bf.knnMatch(des1, des2, k=2)\n",
    "\n",
    "print(type(matches))\n",
    "print(len(matches))\n",
    "print(len(matches[0]))\n",
    "print(matches[1][0].trainIdx)\n",
    "print(matches[1][0].queryIdx)\n",
    "print(matches[6][0].imgIdx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "good = []\n",
    "for m, n in matches:\n",
    "    if m.distance < 0.75 * n.distance:\n",
    "        good.append([m])\n",
    "print(type(good))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "img3 = cv2.drawMatchesKnn(img1,kp1,img2,kp2,good,None,flags=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_show('img3',img3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果需要更快速完成操作，可以尝试使用cv2.FlannBasedMatcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机抽样一致算法（Random sample consensus，RANSAC）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](ransac_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "选择初始样本点进行拟合，给定一个容忍范围，不断进行迭代"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](ransac_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每一次拟合后，容差范围内都有对应的数据点数，找出数据点个数最多的情况，就是最终的拟合结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](ransac_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 单应性矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](ransac_4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}